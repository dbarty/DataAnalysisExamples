{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded4e183-004e-4601-885d-cbe6a3a749f4",
   "metadata": {},
   "source": [
    "# Data Scraper with .csv output\n",
    "Inspired by the YouTube tutorial by Alex The Analyst\n",
    "\n",
    "- [BeautifulSoup + Requests | Web Scraping in Python](https://www.youtube.com/watch?v=bargNl2WeN4)\n",
    "- [Find and Find_All | Web Scraping in Python](https://www.youtube.com/watch?v=xjA1HjvmoMY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee718de-fd8c-4cfb-9312-b080eefe7200",
   "metadata": {},
   "source": [
    "## First try with requests and BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4f90b-d679-4563-80d6-2386edda10e2",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad5433ad-e6d7-4a28-924a-df8067fc249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c245800-ef55-474a-a1ab-743ee33cbaef",
   "metadata": {},
   "source": [
    "### Load one page with 'requests' from URL and check the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e22946-cc0a-442a-9803-df94821b19c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "response = requests.get(url)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fe40a-fde7-4812-935c-4d1f226a7f3b",
   "metadata": {},
   "source": [
    "### Create the BeautifulSoup object and check the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc8f99-0cd4-42be-90b1-5f60bd1b7d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1247d-c698-4678-8724-6e73c95b1118",
   "metadata": {},
   "source": [
    "### Prettify the document data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361a321-9e55-4d6a-93e1-972373c8f64e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d227e5c-32b7-46fc-9757-042f245bb1d1",
   "metadata": {},
   "source": [
    "### Try to fetch the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9a89f32-e667-4560-bf21-53368ebd9532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hockey Teams: Forms, Searching and Pagination | Scrape This Site | A public sandbox for learning web scraping'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"title\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d5712-49e3-4cfc-8ce5-c37eddead27b",
   "metadata": {},
   "source": [
    "### Try to read the pagination links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d815aca3-b601-499e-862d-7a67ece41f52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pagination = soup.find(\"ul\", class_=\"pagination\")\n",
    "for anker in pagination.find_all(\"a\"):\n",
    "    #print(url + \"?\" + anker[\"href\"].split(\"?\")[1])\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a2742-83c6-44b7-9017-32d53f2454d6",
   "metadata": {},
   "source": [
    "### Try to read the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450667c6-b2dd-4fbf-bbc6-2cc5d1df7059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "#print(len(rows))\n",
    "\n",
    "for row in rows:\n",
    "    ths = row.find_all(\"th\")\n",
    "    tds = row.find_all(\"td\")\n",
    "\n",
    "    print([th.text.strip() for th in list(ths)])\n",
    "    print(\"|\".join([td.text.strip() for td in tds]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6bc1f-6953-46e2-9092-4aa12b8cf124",
   "metadata": {},
   "source": [
    "## Let's combine all the parts into our complete scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d00a3-2f9b-4297-9ca7-86d7cd1ec5c5",
   "metadata": {},
   "source": [
    "### Import the modules again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7edb515c-370f-46a8-9fcb-9510a6b58429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e02bf3-4519-4429-bdd7-234380b65c46",
   "metadata": {},
   "source": [
    "### Configure the URL to load and the output filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d97167-ce94-44ed-bc6e-92937a44405d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_filename = \"data/hockey.csv\"\n",
    "\n",
    "url = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "pagination = None\n",
    "\n",
    "headers = []\n",
    "data = []\n",
    "\n",
    "while True:\n",
    "    print(\"Load page:\", url)\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if 200 != response.status_code:  # early exit on error\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html\")  # Create the BeautifulSoup object\n",
    "\n",
    "    # should only run once at the first iteration\n",
    "    if pagination == None:  \n",
    "        ul_pagination = soup.find(\"ul\", class_=\"pagination\")\n",
    "        pagination = [url + \"?\" + anker[\"href\"].split(\"?\")[1] for anker in ul_pagination.find_all(\"a\")][1:-1]\n",
    "\n",
    "    # read table and rows\n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "    rows = table.find_all(\"tr\")\n",
    "\n",
    "    # loop over all rows and extract headers (once) and data\n",
    "    for row in rows:\n",
    "        # should also only run once at the first iteration\n",
    "        ths = row.find_all(\"th\")\n",
    "        if ths and not headers:\n",
    "            headers = [th.text.strip() for th in list(ths)]\n",
    "            continue\n",
    "\n",
    "        # read all colums in a list and append it to data\n",
    "        tds = row.find_all(\"td\")\n",
    "        if tds:  # prevent empty rows\n",
    "            data.append([td.text.strip() for td in tds])\n",
    "\n",
    "    # load the next page\n",
    "    if not pagination:\n",
    "        print(\"No more pages! Have a nice day!\")\n",
    "        break\n",
    " \n",
    "    print(\"Take a little nap!\")\n",
    "    time.sleep(1.5)  # the server owner allows only one request per second \n",
    "    url = pagination.pop(0)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)  # Create a Pandas DataFrame with collected data\n",
    "df.columns = headers  # Set collected headers as column names\n",
    "\n",
    "print(f\"Write to CSV file ({output_filename})\")\n",
    "df.to_csv(output_filename, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a36167-9007-480c-b569-c526717c0bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
